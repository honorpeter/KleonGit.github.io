<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[人工智能路线图]]></title>
    <url>%2F2018%2F12%2Fai%2Freport2018%2F</url>
    <content type="text"><![CDATA[关于深度学习领域相关的算法、系统，还有应用方面的东西。 人工智能这个话题似乎有点儿大。深度学习现在是人工智能领域最重要的一个工具，所以我也趁热学习了一波，把一些我认为有意思的，可能有点用的东西总结在这里。 深度学习的核心肯定是算法模型。如果希望模型预测的更准确，模型的计算速度更快，模型能够在移动设备上运行，就需要对模型的结构做调整，对模型的训练做改进。这就属于算法的科研阶段，只要想出更好的模型结构，能够调出比较理想的参数，在一个公开数据集上获得比较理想的结果，那就可以发论文了，至于论文的影响力，那要看你取得的效果，还有你模型对后来人的启发。 模型的训练和部署需要计算框架的支撑。现有的计算框架基本上都是开源的，它们可以基于CPU，这样的好处就是可以支持任何模型，但是这样效率太低，计算速度太慢，尤其是在做训练的时候，当模型的规模越来越大，我可能需要，几天，几周，甚至几个月才能训好，这个是不能接受的。不过好在英伟达比较厉害，显卡的性能在这几年有了突飞猛进的增长，这才有了今天深度学习的蓬勃发展。 用显卡做训练是毋庸置疑的，但是在做预测的时候功耗很高，所以当需要大规模部署的时候，或者说要放在移动设备上的时候，就需要专用芯片了，性能高功耗低，但是开发周期很长，一旦流片就不能改了，而且开发成本很高，如果卖不出多少货，那就亏了。FPGA就是一个折中的方案，它就是可以改变自己逻辑的一个专用芯片，所以可以在上面实现神经网络的逻辑，但是这样的代价就是它的运行速度和功耗都比专用芯片第一个档次，但是它的开发周期短，可能是专用芯片的1/10，所以如果算法变动很快，那么就需要用更快的开发速度去适应这些算法。 对于一个深度学习的计算框架来说，仅仅有芯片是远远不够的，还需要和芯片配套的软件设施，包括模型的解析，计算图的优化，芯片指令的编译和设备的驱动。广义的来说，CPU和GPU也是计算框架中可用的一种芯片，只不过现在的框架可以通过调用面向CPU和GPU优化的线性代数库，支持大部分的算子。英伟达就需要大量的软件开发人员，优化软件，发挥芯片的性能。 当算法的模型稳定下来之后，当计算框架的易用性和性能提升上来之后，那么基于现有深度学习模型的应用才能大规模的铺开。不像早期的时候，应用多半是偏娱乐性质的，现在人工智能的应用已经产业化，并且规模化了。新一代人工智能开放创新平台，包括百度的自动驾驶，阿里云的城市大脑，腾讯的医疗影像，科大讯飞的智能语音，商汤的智能视觉。而且我们也能实实在在地感受到，人工智能对于我们生活的影响力开始逐渐增大了。安防，交通，医疗，工业，教育，这一次不仅仅是人工智能引起的各个产业的变革，更是借助人工智能的这一波红利，去完成我们之前没有做的传统行业的数字化以及互联网化的工作。 模型 深度学习模型的最基本的思想就是复合多层非线性函数的来拟合任何函数。深度学习的深度就体现在复合函数的层次的深度。深度学习从神经的连接方式中获得启发，其中最早的就是深度神经网络（DNN，Deep Neural Network），又称全连接网络（FCNN，Fully Connected Neural Network），又称。随后，卷积神经网络（CNN，Convolutional Neural Network）和递归神经网络（RNN，Recurrent Neural Network）带来了计算机视觉、语音，以及自然语言处理领域的重大突破。ResNet结构可以避免梯度消失，增加网络的深度。 计算机视觉 计算机视觉领域的五大技术包括： 图像分类 对象检测 目标跟踪 语义分割 实例分割 图像分类 传统的图像分类方法建立图像识别模型，一般包括底层特征学习，特征编码，空间约束分类器设计，模型融合等阶段。而CNN模型（AlexNet）的效果大幅超越了传统方法，随着模型的结构改进，错误率也越来越低，识别能力甚至超过了人眼。 CNN发展2016 CNN包括卷积层，池化层，全连接层，使用Sigmoid、Tanh、ReLu等作为激活函数，可以采用Dropout防止过拟合，使用Batch Normalization（BN）对特征做归一化，加速收敛过程。CNN引入了信号处理领域的概念卷积，使用权值共享与局部感受野大幅降低了参数规模，使得计算得以执行。 CNN架构 图像分类作为计算机视觉的第一步，负责感知图片的结构与特征，通常用作前置骨干网络。主要的模型包括： VGG GoogleNet ResNet MobileNet 常见CNN网络 LeNet LeNet可以说是CNN的开端，包含了CNN的所有基础组件：卷积层，池化层，全连接层，使用Sigmoid作为激活函数。 LeNet网络结构 AlexNet AlexNet结构 AlexNet可以算作CNN的突破，取决于： 非线性激活函数：ReLU 防止过拟合：Dropout，Data augmentation 大数据量训练：百万级ImageNet图像数据 异构计算：GPU LRN(Local Responce Normalization)归一化 VGG VGG相较于AlexNet，采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。但VGG使用3层全连接层，参数规模较大，但收益不高。 VGG结构，VGG16（D），VGG19（E） GoogLeNet Inception V1 GoogLeNet参数较少（AlexNet参数个数是GoogleNet的12倍，VGGNet参数又是AlexNet的3倍），性能较高。GoogLeNet经历了Inception V1到V4的发展。Inception V1通过设计一个稀疏网络结构，但是能够产生稠密的数据，既能增加神经网络表现，又能保证计算资源的使用效率。通过1x1的卷积核，减少计算量 GoogLeNet结构 Inception V2 Inception V2论文主要提出了Batch Normalization，使得训练深度神经网络成为了可能。 在传统机器学习中，对图像提取特征之前，都会对图像做白化操作，即对输入数据变换成0均值、单位方差的正态分布。 卷积神经网络的输入就是图像，白化操作可以加快收敛，对于深度网络，每个隐层的输出都是下一个隐层的输入，即每个隐层的输入都可以做白化操作。 BN可用于加速网络训练，防止梯度消失。如果激活函数是sigmoid，对于每个神经元，可以把逐渐向非线性映射的两端饱和区靠拢的输入分布，强行拉回到0均值单位方差的标准正态分布，即激活函数的兴奋区，在sigmoid兴奋区梯度大，即加速网络训练，还防止了梯度消失。Inception V2的思想同VGG，5x5卷积用3x3卷积代替。 Inception V3 Inception V3引入了： 卷积分解（Factorizing Convolutions），用多个连续小尺寸卷积代替大尺寸卷积。 非对称卷积，nxn的卷积分解成1xn和nx1卷积的串联，减少计算量，提高深度， 降低特征图大小，用并行Conv与Pool代替串联Inception与Pool。 使用Label Smoothing来对网络输出进行正则化。 Inception V3改进的GoogLeNet 上表中的Figure 5指没有进化的Inception，Figure 6是指小卷积版的Inception（用3x3卷积核代替5x5卷积核），Figure 7是指不对称版的Inception（用1xn、nx1卷积核代替nxn卷积核）。 Inception V4 Inception V4研究了Inception模块与残差连接的结合。ResNet结构大大地加深了网络深度，还极大地提升了训练速度，同时性能也有提升。 Inception-ResNet ResNet ResNet基于一个假设：直接映射是难以学习的，不再学习从 x 到 H(x) 的基本映射关系，而是学习这两者之间的差异，也就是残差（residual）。然后，为了计算 H(x)，我们只需要将这个残差加到输入上即可。 ResNet基本结构 ResNet结构示意图 MobileNet 基本假设是：跨通道的相关性和空间相关性是完全可分离的，最好不要联合映射它们。同时期的Xception也有类似发现。 引入了深度可分离卷积（Depthwise Separable Convolution）。Depthwise指不跨通道的卷积，也就是说Feature Map的每个通道有一个独立的卷积核，并且这个卷积核作用且仅作用在这个通道之上。 相较于传统3D卷积，Depthwise卷积如下： 3D卷积与Depthwise卷积对比 Depthwise结合1x1的卷积方式更高效，降低了计算量，并且1x1卷积不需要im2col的数据排布，适合移动端。 MobileNet结构 MobileNet V2是MobileNet的改进版本。引入了Linear bottleneck Inverted Residual Block结构。 Linear bottleneck Inverted Residual Block结构 对象检测 获得了图像的分类，对象检测用来获取对象在图像中的位置，呈现结果为便签+打框。主要分为两大类： 候选区域与分类 RCNN SPP-Net Fast RCNN Faster RCNN R-FCN 端到端 YOLO SSD R-CNN Fast R-CNN Faster R-CNN YOLO SSD RetinaNet 目标跟踪 目标跟踪用于动态识别目标的运动轨迹，比如人脸，车辆，行人。 非常鲁棒的人脸跟踪 目标跟踪算法包括： SiameseFC tracker：end2end离线训练tracker 相关滤波 CFNet MDNet 语义分割 FCN SegNet UNet ENet PSP Net 实例分割 Deep Mask Multi-task Network Cascades(MNC) FCIS Mask RCNN 语音 自然语言处理 系统 深度学习系统正在不断发展与完善之中。利用深度学习框架，就可以可以搭建神经网络，并在CPU与GPU上执行。 主流的计算框架包括： Google家的Tensorflow，背靠TPU Facebook家的PyTorch UCB家的Caffe 这些框架的主要目标平台是GPU，但GPU功耗较高，端到端延迟较大，几乎统治了训练领域。在预测领域，专用芯片可以获得比GPU更高的效率，并且可以根据场景的性能要求与功耗限制定制芯片。 因此适配不同框架到不同的后端设备成了必要的需求。微软、脸书、亚马逊共同推出的ONNX（Open Neural Network Exchange）试图通过统一的Runtime沟通不同的前端框架；NNVM携带自己的TVM试图搭建前端到设备端的编译路径；而Tensorflow自成一体，使用XLA统一中间表示，并适配不同后端设备的优化pass。 框架 深度学习训练与部署的框架百花齐放，以Tensorflow为首，PyTorch，Caffe等紧随其后。各家平台构建网络时特点不同，并不同程度地支持分布式训练。由于社区的开源工作，单机搭建深度学习平台已经相当容易，然而受限于计算资源，难以训练较大规模的模型。对深度学习的训练与部署平台的需求也与日俱增。 除此之外，框架对于底层计算平台的可拓展能力也被看重，Tensorflow的XLA，NNVM的TVM都利用统一的IR与优化pass适配不同的底层硬件。 Tensorflow PyTorch Caffe 设备 深度学习模型训练（Training）与预测（Inference）在CPU上表现欠佳，因而发展出了多种类型的异构计算系统用于加速模型的训练与预测。以GPU为首，NVIDIA推出cuDNN库，作为各种框架的底层平台，已经得到了大规模的应用。除此之外，各类专用芯片也迅速进入异构计算系统中。以FPGA为平台，可以较快适应模型的算法创新，虽然可以获得较高的加速比，但计算效率并不高。ASIC芯片中，以Google的TPU最为出名，同时支持训练与预测。 GPU FPGA ASIC 编译 应用 深度学习目前在图像识别领域最为成熟，各类模型与系统最为丰富。基于图像识别领域的目标检测，语义分割等，支撑了工业检测、安防系统、自动驾驶等多个领域的发展。 自动驾驶 医疗 语音 城市大脑 视觉]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客事记]]></title>
    <url>%2F2018%2F11%2Fhello%2F</url>
    <content type="text"><![CDATA[2018-11-20 博客上线]]></content>
  </entry>
</search>
