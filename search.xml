<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[人工智能路线图]]></title>
    <url>%2F2018%2F12%2Fai%2Freport2018%2F</url>
    <content type="text"><![CDATA[关于深度学习领域相关的算法、系统，还有应用方面的东西。 人工智能这个话题似乎有点儿大。深度学习现在是人工智能领域最重要的一个工具，所以我也趁热学习了一波，把一些我认为有意思的，可能有点用的东西总结在这里。 深度学习的核心肯定是算法模型。如果希望模型预测的更准确，模型的计算速度更快，模型能够在移动设备上运行，就需要对模型的结构做调整，对模型的训练做改进。这就属于算法的科研阶段，只要想出更好的模型结构，能够调出比较理想的参数，在一个公开数据集上获得比较理想的结果，那就可以发论文了，至于论文的影响力，那要看你取得的效果，还有你模型对后来人的启发。 模型的训练和部署需要计算框架的支撑。现有的计算框架基本上都是开源的，它们可以基于CPU，这样的好处就是可以支持任何模型，但是这样效率太低，计算速度太慢，尤其是在做训练的时候，当模型的规模越来越大，我可能需要，几天，几周，甚至几个月才能训好，这个是不能接受的。不过好在英伟达比较厉害，显卡的性能在这几年有了突飞猛进的增长，这才有了今天深度学习的蓬勃发展。 用显卡做训练是毋庸置疑的，但是在做预测的时候功耗很高，所以当需要大规模部署的时候，或者说要放在移动设备上的时候，就需要专用芯片了，性能高功耗低，但是开发周期很长，一旦流片就不能改了，而且开发成本很高，如果卖不出多少货，那就亏了。FPGA就是一个折中的方案，它就是可以改变自己逻辑的一个专用芯片，所以可以在上面实现神经网络的逻辑，但是这样的代价就是它的运行速度和功耗都比专用芯片第一个档次，但是它的开发周期短，可能是专用芯片的1/10，所以如果算法变动很快，那么就需要用更快的开发速度去适应这些算法。 对于一个深度学习的计算框架来说，仅仅有芯片是远远不够的，还需要和芯片配套的软件设施，包括模型的解析，计算图的优化，芯片指令的编译和设备的驱动。广义的来说，CPU和GPU也是计算框架中可用的一种芯片，只不过现在的框架可以通过调用面向CPU和GPU优化的线性代数库，支持大部分的算子。英伟达就需要大量的软件开发人员，优化软件，发挥芯片的性能。 当算法的模型稳定下来之后，当计算框架的易用性和性能提升上来之后，那么基于现有深度学习模型的应用才能大规模的铺开。不像早期的时候，应用多半是偏娱乐性质的，现在人工智能的应用已经产业化，并且规模化了。新一代人工智能开放创新平台，包括百度的自动驾驶，阿里云的城市大脑，腾讯的医疗影像，科大讯飞的智能语音，商汤的智能视觉。而且我们也能实实在在地感受到，人工智能对于我们生活的影响力开始逐渐增大了。安防，交通，医疗，工业，教育，这一次不仅仅是人工智能引起的各个产业的变革，更是借助人工智能的这一波红利，去完成我们之前没有做的传统行业的数字化以及互联网化的工作。 模型深度学习模型的最基本的思想就是复合多层非线性函数的来拟合任何函数。深度学习的深度就体现在复合函数的层次的深度。深度学习从神经的连接方式中获得启发，其中最早的就是深度神经网络（DNN，Deep Neural Network），又称全连接网络（FCNN，Fully Connected Neural Network），又称。随后，卷积神经网络（CNN，Convolutional Neural Network）和递归神经网络（RNN，Recurrent Neural Network）带来了计算机视觉、语音，以及自然语言处理领域的重大突破。ResNet结构可以避免梯度消失，增加网络的深度。 计算机视觉计算机视觉领域的五大技术包括： 图像分类 对象检测 目标跟踪 语义分割 实例分割 图像分类传统的图像分类方法建立图像识别模型，一般包括底层特征学习，特征编码，空间约束分类器设计，模型融合等阶段。而CNN模型（AlexNet）的效果大幅超越了传统方法，随着模型的结构改进，错误率也越来越低，识别能力甚至超过了人眼。 图像分类作为计算机视觉的第一步，负责感知图片的结构与特征，主要的模型包括： CNN VGG GoogleNet ResNet CNNCNN包括卷积层，池化层，全连接层，使用Sigmoid、Tanh、ReLu等作为激活函数，可以采用Dropout防止过拟合，使用Batch Normalization（BN）对特征做归一化，加速收敛过程。CNN引入了信号处理领域的概念卷积，使用权值共享与局部感受野大幅降低了参数规模，使得计算机得以执行。 VGGGoogleNetResNet对象检测得到了图像的分类，下一步是知道对象在图像中的位置。 R-CNNFast R-CNNFaster R-CNNMask R-CNNYOLOSSD语义分割FCN语音自然语言处理 系统深度学习系统正在不断发展与完善之中。利用深度学习框架，就可以可以搭建神经网络，并在CPU与GPU上执行。主流的计算框架包括： Google家的Tensorflow，背靠TPU Facebook家的PyTorch UCB家的Caffe 这些框架的主要目标平台是GPU，但GPU功耗较高，端到端延迟较大，几乎统治了训练领域。在预测领域，专用芯片可以获得比GPU更高的效率，并且可以根据场景的性能要求与功耗限制定制芯片。 因此适配不同框架到不同的后端设备成了必要的需求。微软、脸书、亚马逊共同推出的ONNX（Open Neural Network Exchange）试图通过统一的Runtime沟通不同的前端框架；NNVM携带自己的TVM试图搭建前端到设备端的编译路径；而Tensorflow自成一体，使用XLA统一中间表示，并适配不同后端设备的优化pass。 框架 深度学习训练与部署的框架百花齐放，以Tensorflow为首，PyTorch，Caffe等紧随其后。各家平台构建网络时特点不同，并不同程度地支持分布式训练。由于社区的开源工作，单机搭建深度学习平台已经相当容易，然而受限于计算资源，难以训练较大规模的模型。对深度学习的训练与部署平台的需求也与日俱增。除此之外，框架对于底层计算平台的可拓展能力也被看重，Tensorflow的XLA，NNVM的TVM都利用统一的IR与优化pass适配不同的底层硬件。 设备深度学习模型训练（Training）与预测（Inference）在CPU上表现欠佳，因而发展出了多种类型的异构计算系统用于加速模型的训练与预测。以GPU为首，NVIDIA推出cuDNN库，作为各种框架的底层平台，已经得到了大规模的应用。除此之外，各类专用芯片也迅速进入异构计算系统中。以FPGA为平台，可以较快适应模型的算法创新，虽然可以获得较高的加速比，但计算效率并不高。ASIC芯片中，以Google的TPU最为出名，同时支持训练与预测。 应用深度学习目前在图像识别领域最为成熟，各类模型与系统最为丰富。基于图像识别领域的目标检测，语义分割等，支撑了工业检测、安防系统、自动驾驶等多个领域的发展。 自动驾驶医疗语音城市大脑视觉]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客事记]]></title>
    <url>%2F2018%2F11%2Fhello%2F</url>
    <content type="text"><![CDATA[2018-11-20 博客上线]]></content>
  </entry>
</search>
